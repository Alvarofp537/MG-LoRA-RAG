{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generación de cartas de presentación con LoRA en dos LLMs (Qwen2.5-3B vs Granite/Watson 3B)\n",
        "\n",
        "Este notebook entrena con LoRA dos modelos ~3B en el dataset de cover letters y compara su rendimiento.\n"
      ],
      "metadata": {
        "id": "h9wXbuksfeKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "ds = load_dataset(\"dhruvvaidh/cover-letter-dataset-llama3\")\n",
        "# El dataset expone campos como \"Instruction\", \"Prompt\" y \"Output\"\n",
        "# Unimos en un único prompt-condición y target a generar.\n",
        "\n",
        "def build_example(ex):\n",
        "    # Prompt: combinamos system instruction + user prompt si existen; si no, usamos Instruction y Prompt\n",
        "    instr = ex.get(\"Instruction\", \"\")\n",
        "    user = ex.get(\"Prompt\", \"\")\n",
        "    # Target: Output\n",
        "    target = ex.get(\"Output\", \"\")\n",
        "    # Plantilla simple estilo instruct\n",
        "    merged_prompt = f\"### Instrucción:\\n{instr}\\n\\n### Datos del candidato y puesto:\\n{user}\\n\\n### Respuesta:\"\n",
        "    return {\"text_input\": merged_prompt, \"text_target\": target}\n",
        "\n",
        "ds_proc = ds[\"train\"].map(build_example)\n",
        "# split train/valid\n",
        "train_idx, valid_idx = train_test_split(range(len(ds_proc)), test_size=0.15, random_state=42)\n",
        "train_ds = ds_proc.select(train_idx)\n",
        "valid_ds = ds_proc.select(valid_idx)\n",
        "\n",
        "dataset = DatasetDict({\"train\": train_ds, \"validation\": valid_ds})\n",
        "len(dataset[\"train\"]), len(dataset[\"validation\"])\n"
      ],
      "metadata": {
        "id": "BawFgl99fY8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilidades de tokenización y data collator\n"
      ],
      "metadata": {
        "id": "FMLUmK_PftGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "MAX_LEN = 1024  # suficiente para prompts del dataset\n",
        "\n",
        "def make_tokenize_fn(tokenizer):\n",
        "    def tok_fn(ex):\n",
        "        # Entrenamos causal LM con input+target concatenados; calculamos labels en la parte target\n",
        "        inp = ex[\"text_input\"]\n",
        "        tgt = ex[\"text_target\"]\n",
        "        # Separador claro para delimitar target\n",
        "        full = inp + \"\\n\"\n",
        "        # Tokenizamos por separado para localizar offset\n",
        "        in_ids = tokenizer(full, truncation=True, max_length=MAX_LEN)[\"input_ids\"]\n",
        "        tgt_ids = tokenizer(tgt, truncation=True, max_length=MAX_LEN)[\"input_ids\"]\n",
        "        input_ids = in_ids + tgt_ids + [tokenizer.eos_token_id]\n",
        "        # labels: -100 en la parte del prompt, y etiquetas en la parte target + eos\n",
        "        labels = [-100] * len(in_ids) + tgt_ids + [tokenizer.eos_token_id]\n",
        "        attn = [1] * len(input_ids)\n",
        "        return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": attn}\n",
        "    return tok_fn\n",
        "\n",
        "class DataCollatorForCausalLM:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
        "    def __call__(self, batch):\n",
        "        maxlen = max(len(x[\"input_ids\"]) for x in batch)\n",
        "        def pad(seq, val):\n",
        "            return seq + [val] * (maxlen - len(seq))\n",
        "        input_ids = torch.tensor([pad(x[\"input_ids\"], self.pad_id) for x in batch])\n",
        "        labels = torch.tensor([pad(x[\"labels\"], -100) for x in batch])\n",
        "        attn = torch.tensor([pad(x[\"attention_mask\"], 0) for x in batch])\n",
        "        return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": attn}\n"
      ],
      "metadata": {
        "id": "-ixZ1tA7fvCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuración LoRA común"
      ],
      "metadata": {
        "id": "S5Sz07OzfyNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],  # genérico para LLMs\n",
        "    bias=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "Bvx5ZQaNfyf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento del modelo 1: Qwen2.5-3B (base)\n"
      ],
      "metadata": {
        "id": "MivZ7h5ef05J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "\n",
        "qwen_model_id = \"Qwen/Qwen2.5-3B\"  # base\n",
        "qwen_tok = AutoTokenizer.from_pretrained(qwen_model_id)\n",
        "if qwen_tok.pad_token is None:\n",
        "    qwen_tok.pad_token = qwen_tok.eos_token\n",
        "\n",
        "tok_qwen = dataset.map(make_tokenize_fn(qwen_tok), batched=False)\n",
        "collator_qwen = DataCollatorForCausalLM(qwen_tok)\n",
        "\n",
        "qwen_base = AutoModelForCausalLM.from_pretrained(\n",
        "    qwen_model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "qwen_lora = get_peft_model(qwen_base, lora_config)\n",
        "qwen_lora.print_trainable_parameters()\n",
        "\n",
        "args_qwen = TrainingArguments(\n",
        "    output_dir=\"./qwen3b-lora\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_ratio=0.05,\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    dataloader_num_workers=2,\n",
        "    report_to=[],\n",
        ")\n",
        "trainer_qwen = Trainer(\n",
        "    model=qwen_lora,\n",
        "    args=args_qwen,\n",
        "    train_dataset=tok_qwen[\"train\"],\n",
        "    eval_dataset=tok_qwen[\"validation\"],\n",
        "    data_collator=collator_qwen,\n",
        ")\n",
        "trainer_qwen.train()\n"
      ],
      "metadata": {
        "id": "vzhLYDztf19C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Entrenamiento del modelo 2: Granite/Watson 3B (base)\n"
      ],
      "metadata": {
        "id": "wZbQ1ySkf32J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "granite_model_id = \"ibm-granite/granite-3.1-3b-a800m-instruct\"\n",
        "\n",
        "granite_tok = AutoTokenizer.from_pretrained(granite_model_id)\n",
        "if granite_tok.pad_token is None and granite_tok.eos_token:\n",
        "    granite_tok.pad_token = granite_tok.eos_token\n",
        "\n",
        "tok_granite = dataset.map(make_tokenize_fn(granite_tok), batched=False)\n",
        "collator_granite = DataCollatorForCausalLM(granite_tok)\n",
        "\n",
        "granite_base = AutoModelForCausalLM.from_pretrained(\n",
        "    granite_model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "granite_lora = get_peft_model(granite_base, lora_config)\n",
        "granite_lora.print_trainable_parameters()\n",
        "\n",
        "args_granite = TrainingArguments(\n",
        "    output_dir=\"./granite3b-lora\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_ratio=0.05,\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    dataloader_num_workers=2,\n",
        "    report_to=[],\n",
        ")\n",
        "trainer_granite = Trainer(\n",
        "    model=granite_lora,\n",
        "    args=args_granite,\n",
        "    train_dataset=tok_granite[\"train\"],\n",
        "    eval_dataset=tok_granite[\"validation\"],\n",
        "    data_collator=collator_granite,\n",
        ")\n",
        "trainer_granite.train()\n"
      ],
      "metadata": {
        "id": "_LqFBXCef7XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluación: Perplexity (a partir de pérdida) y ROUGE en validación\n"
      ],
      "metadata": {
        "id": "zdhy4TpNgFDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from evaluate import load as load_metric\n",
        "\n",
        "rouge = load_metric(\"rouge\")\n",
        "bleu = load_metric(\"bleu\")\n",
        "\n",
        "def eval_model(trainer, tokenizer, dataset, n_samples=64, max_new_tokens=400):\n",
        "    # 1) Perplexity a partir de evaluación\n",
        "    metrics = trainer.evaluate()\n",
        "    ppl = math.exp(metrics[\"eval_loss\"]) if \"eval_loss\" in metrics else float(\"nan\")\n",
        "    # 2) Generación y ROUGE/BLEU\n",
        "    preds, refs = [], []\n",
        "    subset = dataset.select(range(min(n_samples, len(dataset))))\n",
        "    model = trainer.model\n",
        "    model.eval()\n",
        "    for ex in subset:\n",
        "        in_ids = tokenizer(ex[\"text_input\"], return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
        "        with torch.no_grad():\n",
        "            gen = model.generate(\n",
        "                **in_ids,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                temperature=0.7,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        text = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
        "        # Extraer la parte de respuesta después de \"### Respuesta:\"\n",
        "        if \"### Respuesta:\" in text:\n",
        "            text = text.split(\"### Respuesta:\")[-1].strip()\n",
        "        preds.append(text)\n",
        "        refs.append(ex[\"text_target\"])\n",
        "    rouge_res = rouge.compute(predictions=preds, references=refs)\n",
        "    # BLEU requiere referencias tokenizadas\n",
        "    bleu_res = bleu.compute(predictions=[p.split() for p in preds],\n",
        "                            references=[[r.split()] for r in refs])\n",
        "    return {\"perplexity\": ppl, \"rougeL\": rouge_res.get(\"rougeL\"), \"bleu\": bleu_res.get(\"bleu\")}\n",
        "\n",
        "res_qwen = eval_model(trainer_qwen, qwen_tok, dataset[\"validation\"])\n",
        "res_granite = eval_model(trainer_granite, granite_tok, dataset[\"validation\"])\n",
        "res_qwen, res_granite\n"
      ],
      "metadata": {
        "id": "NDQkUZpsgHYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparación y ejemplos cualitativos"
      ],
      "metadata": {
        "id": "uSugzCIPgJ0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_metrics = pd.DataFrame([\n",
        "    {\"modelo\": \"Qwen2.5-3B LoRA\", **res_qwen},\n",
        "    {\"modelo\": \"Granite/Watson 3B LoRA\", **res_granite},\n",
        "])\n",
        "df_metrics\n"
      ],
      "metadata": {
        "id": "wPjnKAIFgKDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Muestreo de generaciones para inspección manual"
      ],
      "metadata": {
        "id": "PJule3cmgM6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_generations(trainer, tokenizer, dataset, k=5):\n",
        "    samples = dataset.select(range(min(k, len(dataset))))\n",
        "    outs = []\n",
        "    model = trainer.model\n",
        "    for ex in samples:\n",
        "        in_ids = tokenizer(ex[\"text_input\"], return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
        "        with torch.no_grad():\n",
        "            gen = model.generate(\n",
        "                **in_ids,\n",
        "                max_new_tokens=400,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                temperature=0.7,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        pred = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
        "        if \"### Respuesta:\" in pred:\n",
        "            pred = pred.split(\"### Respuesta:\")[-1].strip()\n",
        "        outs.append({\"prompt\": ex[\"text_input\"], \"target\": ex[\"text_target\"], \"pred\": pred})\n",
        "    return pd.DataFrame(outs)\n",
        "\n",
        "qwen_samples = sample_generations(trainer_qwen, qwen_tok, dataset[\"validation\"])\n",
        "granite_samples = sample_generations(trainer_granite, granite_tok, dataset[\"validation\"])\n",
        "\n",
        "qwen_samples.head(), granite_samples.head()\n"
      ],
      "metadata": {
        "id": "rYgPQkj9gNc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusión\n",
        "# Interpretamos las métricas y las muestras para decidir el modelo con mejor desempeño.\n",
        "# Consideramos: menor perplexity, mayor ROUGE-L/BLEU, y mejor alineación con requisitos del puesto y CV en ejemplos."
      ],
      "metadata": {
        "id": "hpO5vfdTgQQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guardado de adaptadores LoRA\n"
      ],
      "metadata": {
        "id": "6VJIFuZWgR46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Çqwen_lora.save_pretrained(\"./qwen3b-lora-adapter\")\n",
        "granite_lora.save_pretrained(\"./granite3b-lora-adapter\")\n"
      ],
      "metadata": {
        "id": "549bWs-lgQ_B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}