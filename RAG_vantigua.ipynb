{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7oTsr1EmiVF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"BNB_CUDA_VERSION\"] = \"124\"  # force using CUDA 12.4 binary\n",
        "\n",
        "!pip install -U transformers accelerate evaluate\n",
        "!pip install rouge_score sacrebleu\n",
        "!pip uninstall -y bitsandbytes\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnw1FsLPjBB5"
      },
      "source": [
        "# RAG para ranking de candidatos por cover letters\n",
        "# ImplementaciÃ³n 1 (E5 + Qwen2.5-3B) y ImplementaciÃ³n 2 (BGE-large + Granite/Watson 1B)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0XzHroUjF7f"
      },
      "source": [
        "# Carga del dataset y construcciÃ³n del corpus de documentos (cover letters + metadatos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bb7qdOMgi_MU"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "ds = load_dataset(\"dhruvvaidh/cover-letter-dataset-llama3\")\n",
        "# Tomamos el split de train para Ã­ndice y validation para evaluaciÃ³n\n",
        "train_df = ds[\"train\"].to_pandas()\n",
        "valid_df = ds.get(\"validation\", ds[\"train\"]).to_pandas()  # fallback si no hay val split\n",
        "\n",
        "def canonize_row(r):\n",
        "    instr = str(r.get(\"Instruction\", \"\")).strip()\n",
        "    prompt = str(r.get(\"Prompt\", \"\")).strip()     # suele contener job description y/o CV\n",
        "    output = str(r.get(\"Output\", \"\")).strip()     # la cover letter final\n",
        "    # Documento base: la carta + contexto breve del prompt\n",
        "    doc = f\"Cover Letter:\\n{output}\\n\\nContext (job/CV):\\n{prompt}\"\n",
        "    return {\n",
        "        \"doc\": doc,\n",
        "        \"cover_letter\": output,\n",
        "        \"context\": prompt,\n",
        "        \"instruction\": instr\n",
        "    }\n",
        "\n",
        "corpus = train_df.apply(canonize_row, axis=1, result_type=\"expand\")\n",
        "corpus = corpus.reset_index().rename(columns={\"index\": \"doc_id\"})\n",
        "len(corpus)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sb3S0MLjHbG"
      },
      "source": [
        "# Utilidades: normalizaciÃ³n, tokenizaciÃ³n ligera para fragmentaciÃ³n opcional\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "J3covppPjIa_",
        "outputId": "c76a289d-f0b9-43cd-d38d-2c2abbb162bd"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'corpus' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3753207603.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"doc_norm\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"doc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def normalize_text(t):\n",
        "    t = t.replace(\"\\r\",\" \").replace(\"\\n\",\"\\n\")\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "corpus[\"doc_norm\"] = corpus[\"doc\"].apply(normalize_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZNfrn1kjLH-"
      },
      "source": [
        "# ImplementaciÃ³n 1: Embeddings E5 + FAISS y Generador Qwen2.5-3B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewqkMMjqjJ6o"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "embed_model_A_name = \"intfloat/e5-base-v2\"  # Embeddings A\n",
        "embed_model_A = SentenceTransformer(embed_model_A_name)\n",
        "\n",
        "# E5 recomienda prefijos \"query: \" y \"passage: \"\n",
        "docs_A = [\"passage: \" + t for t in corpus[\"doc_norm\"].tolist()]\n",
        "doc_emb_A = embed_model_A.encode(docs_A, batch_size=64, show_progress_bar=True, normalize_embeddings=True)\n",
        "doc_emb_A = np.array(doc_emb_A, dtype=\"float32\")\n",
        "\n",
        "index_A = faiss.IndexFlatIP(doc_emb_A.shape[1])\n",
        "index_A.add(doc_emb_A)\n",
        "\n",
        "# Utilidades de bÃºsqueda\n",
        "def search_A(query, k=5):\n",
        "    q = \"query: \" + query\n",
        "    q_emb = embed_model_A.encode([q], normalize_embeddings=True)\n",
        "    q_emb = np.array(q_emb, dtype=\"float32\")\n",
        "    scores, idxs = index_A.search(q_emb, k)\n",
        "    return [(int(i), float(s)) for i, s in zip(idxs[0], scores[0])]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UagKYrrtjNVG"
      },
      "source": [
        "# Generador A: Qwen2.5-3B (causal LM). Usaremos el modelo base y LoRA si ya estÃ¡ entrenado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gl3X44njPd3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "gen_A_name = \"Qwen/Qwen2.5-3B\"\n",
        "tok_A = AutoTokenizer.from_pretrained(gen_A_name)\n",
        "if tok_A.pad_token is None:\n",
        "    tok_A.pad_token = tok_A.eos_token\n",
        "\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "gen_A = AutoModelForCausalLM.from_pretrained(\n",
        "    gen_A_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "\n",
        "def format_rag_prompt(job_desc, retrieved_docs):\n",
        "    context_str = \"\\n\\n\".join([f\"[Doc {i}] {corpus.iloc[idx]['doc']}\" for i, idx in enumerate(retrieved_docs)])\n",
        "    return (\n",
        "        \"### Tarea:\\n\"\n",
        "        \"Dado un job description, recupera cover letters similares y genera un ranking breve de candidatos con justificaciÃ³n.\\n\\n\"\n",
        "        \"### Job Description:\\n\"\n",
        "        f\"{job_desc}\\n\\n\"\n",
        "        \"### Evidencia recuperada (fragmentos de cover letters):\\n\"\n",
        "        f\"{context_str}\\n\\n\"\n",
        "        \"### Instrucciones:\\n\"\n",
        "        \"- Resume los 3 candidatos mÃ¡s alineados.\\n\"\n",
        "        \"- Fundamenta cada elecciÃ³n con evidencias del contexto recuperado.\\n\"\n",
        "        \"- Tono profesional y conciso.\\n\\n\"\n",
        "        \"### Respuesta:\"\n",
        "    )\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_with_A(job_desc, k=5, max_new_tokens=400, temperature=0.7, top_p=0.9):\n",
        "    hits = search_A(job_desc, k=k)\n",
        "    idxs = [h[0] for h in hits]\n",
        "    prompt = format_rag_prompt(job_desc, idxs)\n",
        "    inputs = tok_A(prompt, return_tensors=\"pt\").to(gen_A.device)\n",
        "    out = gen_A.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_p=top_p, eos_token_id=tok_A.eos_token_id)\n",
        "    text = tok_A.decode(out[0], skip_special_tokens=True)\n",
        "    return text.split(\"### Respuesta:\")[-1].strip(), hits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJJWBIiwjVP4"
      },
      "source": [
        "# ImplementaciÃ³n 2: Embeddings BGE-large + FAISS y Generador Granite/Watson 3B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX4U2yq1jTNf"
      },
      "outputs": [],
      "source": [
        "embed_model_B_name = \"BAAI/bge-large-en-v1.5\"  # Embeddings B\n",
        "embed_model_B = SentenceTransformer(embed_model_B_name)\n",
        "\n",
        "# BGE suele usar prompt \"Represent this passage for retrieval: \"\n",
        "docs_B = [\"Represent this passage for retrieval: \" + t for t in corpus[\"doc_norm\"].tolist()]\n",
        "doc_emb_B = embed_model_B.encode(docs_B, batch_size=32, show_progress_bar=True, normalize_embeddings=True)\n",
        "doc_emb_B = np.array(doc_emb_B, dtype=\"float32\")\n",
        "\n",
        "index_B = faiss.IndexFlatIP(doc_emb_B.shape[1])\n",
        "index_B.add(doc_emb_B)\n",
        "\n",
        "def search_B(query, k=5):\n",
        "    q = \"Represent this query for retrieving relevant passages: \" + query\n",
        "    q_emb = embed_model_B.encode([q], normalize_embeddings=True)\n",
        "    q_emb = np.array(q_emb, dtype=\"float32\")\n",
        "    scores, idxs = index_B.search(q_emb, k)\n",
        "    return [(int(i), float(s)) for i, s in zip(idxs[0], scores[0])]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzicM2bBjY1X"
      },
      "source": [
        "# Generador B: Granite/Watson 3B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMUJQAFNjZK3"
      },
      "outputs": [],
      "source": [
        "gen_B_name = \"ibm-granite/granite-3.1-1b-a400m-instruct\"\n",
        "tok_B = AutoTokenizer.from_pretrained(gen_B_name)\n",
        "if tok_B.pad_token is None and tok_B.eos_token:\n",
        "    tok_B.pad_token = tok_B.eos_token\n",
        "\n",
        "gen_B = AutoModelForCausalLM.from_pretrained(gen_B_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "\n",
        "def format_rag_prompt_B(job_desc, retrieved_docs):\n",
        "    context_str = \"\\n\\n\".join([f\"[Doc {i}] {corpus.iloc[idx]['doc']}\" for i, idx in enumerate(retrieved_docs)])\n",
        "    return (\n",
        "        \"Task: Rank candidates based on fit to the job description using retrieved cover letters as evidence.\\n\\n\"\n",
        "        f\"Job Description:\\n{job_desc}\\n\\n\"\n",
        "        f\"Retrieved Evidence:\\n{context_str}\\n\\n\"\n",
        "        \"Instructions:\\n- Provide top-3 candidates with 1-2 evidence points each.\\n- Keep it concise and professional.\\n\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_with_B(job_desc, k=5, max_new_tokens=400, temperature=0.7, top_p=0.9):\n",
        "    hits = search_B(job_desc, k=k)\n",
        "    idxs = [h[0] for h in hits]\n",
        "    prompt = format_rag_prompt_B(job_desc, idxs)\n",
        "    inputs = tok_B(prompt, return_tensors=\"pt\").to(gen_B.device)\n",
        "    out = gen_B.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_p=top_p, eos_token_id=tok_B.eos_token_id)\n",
        "    text = tok_B.decode(out[0], skip_special_tokens=True)\n",
        "    return text.split(\"Answer:\")[-1].strip(), hits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucGfbGj9jf5P"
      },
      "source": [
        "# EvaluaciÃ³n de recuperaciÃ³n: Precision@k y nDCG@k (queries tomadas de prompts de validaciÃ³n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EkMOHESjgle"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ndcg_score\n",
        "import numpy as np\n",
        "\n",
        "# Construimos queries: tomamos job descriptions (Prompt) y asumimos que el \"Output\" correspondiente es relevante.\n",
        "eval_df = valid_df.copy()\n",
        "eval_df[\"query\"] = eval_df[\"Prompt\"].astype(str)\n",
        "\n",
        "def build_relevance_vector(hit_indices, gold_index, k):\n",
        "    rel = np.zeros(k)\n",
        "    for i, idx in enumerate(hit_indices[:k]):\n",
        "        if idx == gold_index:\n",
        "            rel[i] = 1.0\n",
        "    return rel\n",
        "\n",
        "def evaluate_retrieval(search_fn, name, n_cases=50, k=5):\n",
        "    qs = eval_df[\"query\"].tolist()[:n_cases]\n",
        "    # Mapeo del \"gold\" a Ã­ndice de corpus por emparejamiento simple (puente doc_id vÃ­a train_df)\n",
        "    # SimplificaciÃ³n: si hay correspondencia exacta de 'Output' en corpus, usamos su Ã­ndice; si no, rel=0.\n",
        "    gold_texts = eval_df[\"Output\"].astype(str).tolist()[:n_cases]\n",
        "    gold_map = {corpus.iloc[i][\"cover_letter\"]: i for i in range(len(corpus))}\n",
        "    precs, ndcgs = [], []\n",
        "    for q, gold in zip(qs, gold_texts):\n",
        "        hits = search_fn(q, k=k)\n",
        "        idxs = [h[0] for h in hits]\n",
        "        # relevancias\n",
        "        gold_idx = gold_map.get(gold, None)\n",
        "        rel = build_relevance_vector(idxs, gold_idx, k)\n",
        "        prec = rel.sum() / k\n",
        "        score = ndcg_score([rel], [rel])\n",
        "        precs.append(prec)\n",
        "        ndcgs.append(score)\n",
        "    return {\"name\": name, \"precision@{}\".format(k): float(np.mean(precs)), \"ndcg@{}\".format(k): float(np.mean(ndcgs))}\n",
        "\n",
        "ret_A = evaluate_retrieval(lambda q: search_A(q, k=5), \"E5 + FAISS\", n_cases=50, k=5)\n",
        "ret_B = evaluate_retrieval(lambda q: search_B(q, k=5), \"BGE-large + FAISS\", n_cases=50, k=5)\n",
        "ret_A, ret_B\n",
        "## Max precision: 1/k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkEuYPPzjobm"
      },
      "source": [
        "# Muestreo cualitativo: ver grounding y evidencia citada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f2l9AgYjmom"
      },
      "outputs": [],
      "source": [
        "samples = [\n",
        "    \"Senior Data Scientist with NLP focus, experience in transformer-based models, Python, and cloud (AWS/GCP).\",\n",
        "    \"Frontend Engineer (React/TypeScript) with UX emphasis, accessibility, and design systems.\"\n",
        "]\n",
        "rows = []\n",
        "for s in samples:\n",
        "    pred_A, hits_A = generate_with_A(s, k=5)\n",
        "    pred_B, hits_B = generate_with_B(s, k=5)\n",
        "    rows.append({\n",
        "        \"query\": s,\n",
        "        \"A_pred\": pred_A,\n",
        "        \"A_hits\": hits_A,\n",
        "        \"B_pred\": pred_B,\n",
        "        \"B_hits\": hits_B\n",
        "    })\n",
        "pd.DataFrame(rows)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
