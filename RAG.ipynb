{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#  INSTALACIÓN Y CARGA DE DEPENDENCIAS\n!pip install transformers sentence-transformers accelerate numpy pandas pyarrow datasets torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:47:26.309882Z","iopub.execute_input":"2025-11-24T19:47:26.310092Z","iopub.status.idle":"2025-11-24T19:47:30.359201Z","shell.execute_reply.started":"2025-11-24T19:47:26.310076Z","shell.execute_reply":"2025-11-24T19:47:30.358417Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (22.0.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.4.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.15.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install faiss-cpu bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:47:30.360275Z","iopub.execute_input":"2025-11-24T19:47:30.360533Z","iopub.status.idle":"2025-11-24T19:47:33.878059Z","shell.execute_reply.started":"2025-11-24T19:47:30.360507Z","shell.execute_reply":"2025-11-24T19:47:33.877069Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.13.0)\nRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.48.2)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Es necesario generar un token de hugginface y darle permisos de lectura para poder acceder al repositorio de mistral.<br>\nUna vez creado el token hay q añadirlo a secrets de kagle bajo el nombre de HF_TOKEN","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nif hf_token:\n    login(token=hf_token, add_to_git_credential=False)\n    print(\"Login en Hugging Face exitoso.\")\nelse:\n    raise \"Error token no definido\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:47:33.879274Z","iopub.execute_input":"2025-11-24T19:47:33.880292Z","iopub.status.idle":"2025-11-24T19:47:34.085344Z","shell.execute_reply.started":"2025-11-24T19:47:33.880264Z","shell.execute_reply":"2025-11-24T19:47:34.084331Z"}},"outputs":[{"name":"stdout","text":"Login en Hugging Face exitoso.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ==============================================================================\n#  DEPENDENCIAS Y LIBRERÍAS\n# ==============================================================================\nimport pandas as pd\nimport numpy as np\nimport torch\nimport faiss\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\nimport warnings\nwarnings.filterwarnings(\"ignore\") # Ignorar warnings de tokenizers y modelos\n\n\n# ==============================================================================\n# CARGA Y PROCESAMIENTO DEL CORPUS\n# ==============================================================================\ndef cargar_corpus():\n    print(\"--- 1. Cargando Dataset y Corpus ---\")\n    ds = load_dataset(\"dhruvvaidh/cover-letter-dataset-llama3\")\n    train_df = ds[\"train\"].to_pandas()\n\n    def canonize_row(r):\n        output = str(r.get(\"Output\", \"\")).strip()\n        return {\n            \"doc_id\": r.name,\n            \"text_for_rag\": output,\n        }\n\n    corpus_df = train_df.apply(canonize_row, axis=1, result_type=\"expand\")\n    corpus_list = corpus_df.to_dict('records')\n    textos_para_indexar = corpus_df['text_for_rag'].tolist()\n    \n    print(f\"Corpus cargado: {len(corpus_list)} documentos.\")\n    return corpus_list, textos_para_indexar\n\ncorpus_list, textos_para_indexar = cargar_corpus()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:48:54.416990Z","iopub.execute_input":"2025-11-24T19:48:54.417303Z","iopub.status.idle":"2025-11-24T19:48:55.206622Z","shell.execute_reply.started":"2025-11-24T19:48:54.417281Z","shell.execute_reply":"2025-11-24T19:48:55.205848Z"}},"outputs":[{"name":"stdout","text":"--- 1. Cargando Dataset y Corpus ---\nCorpus cargado: 813 documentos.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Modelo A: Uso de BGE para retrieval y uso de QWEN para generation","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# INDEXACIÓN (modelo BGE + FAISS vectorial) \n# ==============================================================================\nprint(\"\\n--- Generando Embeddings e Índice FAISS (BGE) ---\")\nEMBED_MODEL_ID = \"BAAI/bge-large-en-v1.5\"\n\nembedder_A = SentenceTransformer(EMBED_MODEL_ID)\ndoc_embeddings = embedder_A.encode(\n    textos_para_indexar, \n    batch_size=32, \n    show_progress_bar=True, \n    normalize_embeddings=True\n)\ndoc_embeddings = np.array(doc_embeddings, dtype=\"float32\")\nd_dimension = doc_embeddings.shape[1]\n\nindex = faiss.IndexFlatIP(d_dimension)\nindex.add(doc_embeddings)\nprint(f\"Índice FAISS creado en CPU con {index.ntotal} vectores.\")\n\n\n# ==============================================================================\n# FUNCIÓN DE BÚSQUEDA \n# ==============================================================================\ndef buscar_candidatos_A(query, k=3):\n    \n    q_text = \"Represent this sentence for searching relevant passages: \" + query\n    q_emb = embedder_A.encode([q_text], normalize_embeddings=True)\n    q_emb = np.array(q_emb, dtype=\"float32\")\n    \n    scores, indices = index.search(q_emb, k)\n    \n    resultados = []\n    for idx, score in zip(indices[0], scores[0]):\n        if idx != -1:\n            doc_data = corpus_list[idx]\n            resultados.append({\n                \"id\": doc_data['doc_id'],\n                \"score\": float(score),\n                \"context\": doc_data['text_for_rag'], # Cover Letter completa\n            })\n    return resultados","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:48:55.342882Z","iopub.execute_input":"2025-11-24T19:48:55.343462Z","iopub.status.idle":"2025-11-24T19:49:11.699283Z","shell.execute_reply.started":"2025-11-24T19:48:55.343437Z","shell.execute_reply":"2025-11-24T19:49:11.698624Z"}},"outputs":[{"name":"stdout","text":"\n--- Generando Embeddings e Índice FAISS (BGE) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81f983f987d0488b9b85cc0c6c075f53"}},"metadata":{}},{"name":"stdout","text":"Índice FAISS creado en CPU con 813 vectores.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ==============================================================================\n#  CONFIGURACIÓN DEL GENERATOR Qwen\n# ==============================================================================\nLLM_ID = \"Qwen/Qwen2.5-3B\"\nprint(f\"\\n---  Cargando LLM Generador: {LLM_ID} (Contexto 128K tokens) ---\")\n\n# Configuración de 4 bits para ahorrar memoria\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\ntokenizer_A = AutoTokenizer.from_pretrained(LLM_ID)\nllm_model = AutoModelForCausalLM.from_pretrained(\n    LLM_ID,\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\n\n# Configuración del tokenizers para generación causal\ntokenizer_A.pad_token = tokenizer_A.eos_token \ntokenizer_A.padding_side = \"left\" \n\nranker_pipeline_A = pipeline(\n    \"text-generation\", \n    model=llm_model,\n    tokenizer=tokenizer_A,\n    device_map=\"auto\"\n)\n\nprint(\"LLM cargado correctamente en memoria reducida (4-bit).\")\n\n\n# ==============================================================================\n#  FUNCIÓN DE RANKING\n# ==============================================================================\ndef generar_ranking_llm_A(job_offer, candidatos):\n    \"\"\"Genera un ranking usando Qwen2.5-3B\"\"\"\n    \n    if not ranker_pipeline_A:\n        return \"Error: LLM no disponible.\"\n    \n    # 1. Construir el texto de los candidatos\n    contexto_str = \"\"\n    for i, c in enumerate(candidatos, 1):\n        perfil = c['context'].replace(\"\\n\", \" \").strip()\n        contexto_str += f\"CANDIDATE {i} (ID {c['id']}) - COVER LETTER: {perfil}\\n\\n\"\n        \n    contexto_str = \"\"\n    for i, c in enumerate(candidatos, 1):\n        perfil = c['context'].replace(\"\\n\", \" \").strip()\n        contexto_str += f\"CANDIDATE PROFILE (ID {c['id']}): {perfil}\\n---\\n\"\n        \n    messages = [\n            {\n                \"role\": \"system\",\n                \"content\": (\n                    \"You are a decisive Technical Recruiter. \"\n                    \"Your ONLY task is to select and justify the SINGLE BEST candidate for the job on how well their application matches the provided Job Description. \"\n                    \"Under NO circumstances should you mention, rank, or evaluate any other candidate.\"\n                )\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n        ### JOB DESCRIPTION:\n        {job_offer}\n        \n        ### CANDIDATES LIST:\n        {contexto_str}\n        \n        ### TASKS:\n        Based on the profiles above, identify the single best match for the Job Description. \n        Begin your output immediately with the REQUIRED OUTPUT FORMAT and then STOP WRITING.\n        \n        ### REQUIRED OUTPUT FORMAT (ONLY ONE CANDIDATE):\n        **WINNER ID:** [ID of the best candidate]\n        **MATCH SCORE:** [1-10]\n        **DECISION RATIONALE:** [4-6 lines explaining why this candidate is the best fit, mentioning specific skills from their text that match the JD.]\n        \"\"\"\n            }\n    ]\n    \n    # 3. Aplicar plantilla de chat\n    prompt = ranker_pipeline_A.tokenizer.apply_chat_template(\n        messages, \n        tokenize=False, \n        add_generation_prompt=True\n    )\n    \n    # 4. Generación\n    outputs = ranker_pipeline_A(\n        prompt, \n        max_new_tokens=512,  \n        temperature=0.2,     # Temperatura baja para ser más analítico y menos creativo\n        do_sample=True,\n        return_full_text=False\n    )\n    \n    return outputs[0]['generated_text'].strip()\n\n# ==============================================================================\n# FUNCIÓN RAG AUTOMATIZADA \n# ==============================================================================\ndef ejecutar_rag_pipeline_A(job_offer_query, k=3):\n    \"\"\"Ejecuta el pipeline RAG completo para encontrar y rankear candidatos.\"\"\"\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"INICIANDO RAG para: {job_offer_query}\")\n    print(f\"Buscando los {k} mejores candidatos (usando Cover Letters completas)...\")\n    print(f\"{'='*60}\")\n\n    # 1. RECUPERACIÓN (Retrieval - BGE + FAISS)\n    candidatos_encontrados = buscar_candidatos_A(job_offer_query, k=k)\n\n    if not candidatos_encontrados:\n        print(\"\\n No se encontraron candidatos relevantes. Terminando el pipeline.\")\n        return { \"ranking_final_llm\": \"No se encontraron candidatos para rankear.\" }\n\n    print(\"\\n[FASE 1: RECUPERACIÓN COMPLETADA]\")\n    for i, c in enumerate(candidatos_encontrados, 1):\n        print(f\"  {i}. Candidato ID: {c['id']} | Similitud BGE: {c['score']:.4f}\")\n\n    # 2. RANKING/GENERACIÓN \n    print(f\"\\n{'-'*60}\")\n    print(\"INICIANDO FASE DE RANKING (LLM)...\")\n    \n    ranking_generado = generar_ranking_llm_A(job_offer_query, candidatos_encontrados)\n    \n    print(f\"{'-'*60}\")\n    print(\"REPORTE FINAL DE RR.HH:\")\n    print(ranking_generado)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T20:06:00.722110Z","iopub.execute_input":"2025-11-24T20:06:00.722864Z","iopub.status.idle":"2025-11-24T20:06:10.804397Z","shell.execute_reply.started":"2025-11-24T20:06:00.722826Z","shell.execute_reply":"2025-11-24T20:06:10.803638Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modelo B: Uso de E5 para retrieval y uso de Phi-3-mini para generation","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# INDEXACIÓN (Modelo E5 + FAISS vectorial) \n# ==============================================================================\nprint(\"\\n--- Generando Embeddings e Índice FAISS (E5) ---\")\nEMBED_MODEL_ID = \"intfloat/e5-large-v2\"\n\nembedder_B = SentenceTransformer(EMBED_MODEL_ID)\n\n#  E5 necesita que los documentos lleven el prefijo \"passage: \" \ntextos_con_prefijo = [f\"passage: {t}\" for t in textos_para_indexar]\n\ndoc_embeddings = embedder_B.encode(\n    textos_con_prefijo, \n    batch_size=32, \n    show_progress_bar=True, \n    normalize_embeddings=True\n)\ndoc_embeddings = np.array(doc_embeddings, dtype=\"float32\")\nd_dimension = doc_embeddings.shape[1]\n\nindex = faiss.IndexFlatIP(d_dimension)\nindex.add(doc_embeddings)\nprint(f\"Índice FAISS creado en CPU con {index.ntotal} vectores usando E5.\")\n\n# ==============================================================================\n# FUNCIÓN DE BÚSQUEDA \n# ==============================================================================\ndef buscar_candidatos_B(query, k=3):\n    # E5 necesita que la query lleve el prefijo \"query: \"\n    q_text = f\"query: {query}\"\n    q_emb = embedder_B.encode([q_text], normalize_embeddings=True)\n    q_emb = np.array(q_emb, dtype=\"float32\")\n    \n    scores, indices = index.search(q_emb, k)\n    \n    resultados = []\n    for idx, score in zip(indices[0], scores[0]):\n        if idx != -1:\n            doc_data = corpus_list[idx]\n            resultados.append({\n                \"id\": doc_data['doc_id'],\n                \"score\": float(score),\n                \"context\": doc_data['text_for_rag'], \n            })\n    return resultados","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:49:54.465914Z","iopub.execute_input":"2025-11-24T19:49:54.466234Z","iopub.status.idle":"2025-11-24T19:50:19.275410Z","shell.execute_reply.started":"2025-11-24T19:49:54.466215Z","shell.execute_reply":"2025-11-24T19:50:19.274777Z"}},"outputs":[{"name":"stdout","text":"\n--- Generando Embeddings e Índice FAISS (E5) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae6caeaea0ee423c8d1b46df6cece4cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"544925a4edf64344b591cb70b63e28f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2f5ce615b7448eaba855e356ff34815"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b863c4e6fc844e6b42e23de74ee1453"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff4a07d5e92f4836bd98ec43e65e5a3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54beb325a9344ffea78f0b5e228a8d6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4c5085b0bff457faac6b4b0af8c5ade"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79ac3832f0fe4f72a6733d9635df3ead"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed2ed8b7af584e50a9bdbcb020c62f53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"188b5cfce74d4a9297460d0ba5e212bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7d8c927d7004674b30e8b4a1bc973d8"}},"metadata":{}},{"name":"stdout","text":"Índice FAISS creado en CPU con 813 vectores usando E5.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ==============================================================================\n#  CONFIGURACIÓN DEL GENERATOR phi3\n# ==============================================================================\nLLM_ID = \"microsoft/Phi-3-mini-4k-instruct\"\nprint(f\"\\n--- Cargando LLM Generador: {LLM_ID} (Contexto 32K tokens) ---\")\n\n# Configuración de 4 bits para ahorrar memoria\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\ntokenizer_B = AutoTokenizer.from_pretrained(LLM_ID)\nllm_model = AutoModelForCausalLM.from_pretrained(\n    LLM_ID,\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\n\n# Configuración del tokenizers para generación causal\ntokenizer_B.pad_token = tokenizer_B.eos_token \ntokenizer_B.padding_side = \"left\" \n\nranker_pipeline_B = pipeline(\n    \"text-generation\", \n    model=llm_model,\n    tokenizer=tokenizer_B,\n    device_map=\"auto\"\n)\n\nprint(\"LLM cargado correctamente en memoria reducida (4-bit).\")\n\n\n# ==============================================================================\n#  FUNCIÓN DE RANKING\n# ==============================================================================\ndef generar_ranking_llm_B(job_offer, candidatos):\n    \"\"\"Genera un ranking usando phi3\"\"\"\n    \n    if not ranker_pipeline_B:\n        return \"Error: LLM no disponible.\"\n    \n    # 1. Construir el texto de los candidatos\n    contexto_str = \"\"\n    for i, c in enumerate(candidatos, 1):\n        perfil = c['context'].replace(\"\\n\", \" \").strip()\n        contexto_str += f\"CANDIDATE {i} (ID {c['id']}) - COVER LETTER: {perfil}\\n\\n\"\n    \n    # 2. Definición de mensajes\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": (\n                \"You are an expert Technical Recruiter. \"\n                \"Your sole task is to identify the SINGLE BEST candidate for the job from the provided list \"\n                \"on how well their application matches the provided Job Description. \"\n                \"Do not waste tokens analyzing candidates that do not fit.\"\n            )\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"\n        ### JOB DESCRIPTION:\n        {job_offer}\n        \n        ### CANDIDATES LIST:\n        {contexto_str}\n        \n        ### TASK:\n        Analyze the candidates and select ONLY the #1 Best Fit for this job.\n        \n        ### REQUIRED OUTPUT FORMAT:\n        You must provide the output strictly in this format:\n        \n        **WINNER ID:** [ID]\n        **MATCH SCORE:** [Give a score 1-10 based on the JD]\n        **DECISION RATIONALE:** [Write a concise paragraph (3-5 lines) explaining why this candidate is the best fit. explicitly mention the matching hard skills (e.g. Python, AWS) found in their text that match the Job Description.]\n        (Do NOT copy the full cover letter. Only provide the ranking and the reasoning).\n        \"\"\"\n        }\n    ]\n\n    # 3. Aplicar plantilla de chat\n    prompt = ranker_pipeline_B.tokenizer.apply_chat_template(\n        messages, \n        tokenize=False, \n        add_generation_prompt=True\n    )\n    \n    # 4. Generación\n    outputs = ranker_pipeline_B(\n        prompt, \n        max_new_tokens=512,  \n        temperature=0.2,     # Temperatura baja para ser más analítico y menos creativo\n        do_sample=True,\n        return_full_text=False\n    )\n    \n    return outputs[0]['generated_text'].strip()\n\n\n# ==============================================================================\n# FUNCIÓN RAG AUTOMATIZADA \n# ==============================================================================\ndef ejecutar_rag_pipeline_B(job_offer_query, k=3):\n    \"\"\"Ejecuta el pipeline RAG completo para encontrar y rankear candidatos.\"\"\"\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"INICIANDO RAG para: {job_offer_query}\")\n    print(f\"Buscando los {k} mejores candidatos (usando Cover Letters completas)...\")\n    print(f\"{'='*60}\")\n\n    # 1. RECUPERACIÓN (Retrieval - BGE + FAISS)\n    candidatos_encontrados = buscar_candidatos_B(job_offer_query, k=k)\n\n    if not candidatos_encontrados:\n        print(\"\\n No se encontraron candidatos relevantes. Terminando el pipeline.\")\n        return { \"ranking_final_llm\": \"No se encontraron candidatos para rankear.\" }\n\n    print(\"\\n[FASE 1: RECUPERACIÓN COMPLETADA]\")\n    for i, c in enumerate(candidatos_encontrados, 1):\n        print(f\"  {i}. Candidato ID: {c['id']} | Similitud E5: {c['score']:.4f}\")\n\n    # 2. RANKING/GENERACIÓN \n    print(f\"\\n{'-'*60}\")\n    print(\"INICIANDO FASE DE RANKING (LLM)...\")\n    \n    ranking_generado = generar_ranking_llm_B(job_offer_query, candidatos_encontrados)\n    \n    print(f\"{'-'*60}\")\n    print(\"REPORTE FINAL DE RR.HH:\")\n    print(ranking_generado)\n\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:50:19.276438Z","iopub.execute_input":"2025-11-24T19:50:19.276683Z","iopub.status.idle":"2025-11-24T19:50:59.628275Z","shell.execute_reply.started":"2025-11-24T19:50:19.276666Z","shell.execute_reply":"2025-11-24T19:50:59.627644Z"}},"outputs":[{"name":"stdout","text":"\n--- Cargando LLM Generador: microsoft/Phi-3-mini-4k-instruct (Contexto 32K tokens) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40d2081630a54a47a02f327637f45304"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2017e10e32b4ecf9d715fcbcdcd2477"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96c564304e7a4d41a59cd5eea11eced4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"995aa100205a4e5d87c6c4a1730c3883"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca357831f79f4108b1018f5246ede82e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1855564534434833b1de4005de4280d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"791b7c8b19a94760818d6a9b97db9f61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dce0d10dedd4a1d9f399da104f5d8df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f36eea914f9a459a9d5c5e64e2d2db53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b77f2fc6c65048e596b8ed99dce36913"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea90f00b93ba43aca068f60caf56c31e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc1009bbc3e94a2b82762ce8dab99824"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"LLM cargado correctamente en memoria reducida (4-bit).\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## Resultados","metadata":{}},{"cell_type":"code","source":"TARGET_JOB = \"We need a Project Manager with AWS certification, strong leadership, experience in other projects related to finances\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:50:59.629400Z","iopub.execute_input":"2025-11-24T19:50:59.629679Z","iopub.status.idle":"2025-11-24T19:50:59.633598Z","shell.execute_reply.started":"2025-11-24T19:50:59.629660Z","shell.execute_reply":"2025-11-24T19:50:59.632622Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# usando BGE + QWEN\nejecutar_rag_pipeline_A(TARGET_JOB, k=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T20:06:10.805514Z","iopub.execute_input":"2025-11-24T20:06:10.806167Z","iopub.status.idle":"2025-11-24T20:06:22.518432Z","shell.execute_reply.started":"2025-11-24T20:06:10.806147Z","shell.execute_reply":"2025-11-24T20:06:22.517574Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nINICIANDO RAG para: We need a Project Manager with AWS certification, strong leadership, experience in other projects related to finances\nBuscando los 3 mejores candidatos (usando Cover Letters completas)...\n============================================================\n\n[FASE 1: RECUPERACIÓN COMPLETADA]\n  1. Candidato ID: 810 | Similitud BGE: -0.0083\n  2. Candidato ID: 778 | Similitud BGE: -0.0114\n  3. Candidato ID: 117 | Similitud BGE: -0.0116\n\n------------------------------------------------------------\nINICIANDO FASE DE RANKING (LLM)...\n------------------------------------------------------------\nREPORTE FINAL DE RR.HH:\n---\n\nAssistant: **WINNER ID:** 778\n**MATCH SCORE:** 9\n**DECISION RATIONALE:** The candidate with ID 778 has 8 years of experience in mobile app development, which is more than the required 5 years mentioned in the job description. Additionally, they have a Master's degree in Computer Science, which is a strong requirement for the position. They are proficient in iOS, Android, Swift, Java, and Kotlin, which are all relevant technologies for mobile app development. Furthermore, they possess excellent communication skills and the ability to work independently, which are essential qualities for a project manager. Overall, their extensive experience and qualifications make them the best fit for this role.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# usando E5 + phi3\nejecutar_rag_pipeline_B(TARGET_JOB, k=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:53:25.789215Z","iopub.execute_input":"2025-11-24T19:53:25.789848Z","iopub.status.idle":"2025-11-24T19:53:31.737409Z","shell.execute_reply.started":"2025-11-24T19:53:25.789820Z","shell.execute_reply":"2025-11-24T19:53:31.736543Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nINICIANDO RAG para: We need a Project Manager with AWS certification, strong leadership, experience in other projects related to finances\nBuscando los 3 mejores candidatos (usando Cover Letters completas)...\n============================================================\n\n[FASE 1: RECUPERACIÓN COMPLETADA]\n  1. Candidato ID: 409 | Similitud E5: 0.7958\n  2. Candidato ID: 193 | Similitud E5: 0.7947\n  3. Candidato ID: 171 | Similitud E5: 0.7940\n\n------------------------------------------------------------\nINICIANDO FASE DE RANKING (LLM)...\n------------------------------------------------------------\nREPORTE FINAL DE RR.HH:\n**WINNER ID:** 171\n\n**MATCH SCORE:** 9\n\n**DECISION RATIONALE:** Candidate 171 is the best fit for the Project Manager position with AWS certification and finance-related project experience. The cover letter explicitly mentions proficiency in SQL, Python, Scala, and AWS services, aligning closely with the job requirements. The candidate'emen\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}