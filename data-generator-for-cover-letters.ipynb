{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"95e6f67e","cell_type":"markdown","source":"# Generador de Datos Sintéticos con LLaMA-2\n\nEste script implementa un **generador de datos sintéticos** basado en prompts generados también de forma **sintética**, con el objetivo de **incrementar la diversidad** de los textos generados.\n\n **Requisitos importantes**:\n\n1. **Token de Hugging Face**: Es necesario tener un token de acceso válido a Hugging Face.  \n2. **Aceptación de Términos y Condiciones**: Debes aceptar los términos de uso de los modelos de **Meta-LLaMA** para poder descargar y usar los checkpoints.\n\n---\n\n## Configuración del token\n\nCrea un archivo llamado `secrets.json` en el mismo directorio del proyecto con la siguiente estructura:\n\n```json\n{\n    \"huggingface_token\": \"TU_TOKEN_AQUI\"\n}\n","metadata":{}},{"id":"f91885b4-894e-4a64-94d8-d2e184963c65","cell_type":"code","source":"!git clone https://github.com/bitsandbytes-foundation/bitsandbytes.git\n%cd bitsandbytes\n!pip install cmake\n!cmake -DCOMPUTE_BACKEND=cuda -S . -B build\n!cmake --build build --config Release\n!pip install .\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T23:31:33.720424Z","iopub.execute_input":"2025-11-18T23:31:33.721290Z","iopub.status.idle":"2025-11-18T23:31:38.663159Z","shell.execute_reply.started":"2025-11-18T23:31:33.721259Z","shell.execute_reply":"2025-11-18T23:31:38.662149Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'bitsandbytes' already exists and is not an empty directory.\n/kaggle/working/bitsandbytes/bitsandbytes\nRequirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (3.31.6)\n\u001b[0mCMake Error: The source directory \"/kaggle/working/bitsandbytes/bitsandbytes\" does not appear to contain CMakeLists.txt.\nSpecify --help for usage, or press the help button on the CMake GUI.\u001b[0m\nError: /kaggle/working/bitsandbytes/bitsandbytes/build is not a directory\n\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":6},{"id":"bf945111","cell_type":"code","source":"!pip install transformers torch\n# !pip install -U bitsandbytes --no-deps","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T23:29:56.373569Z","iopub.execute_input":"2025-11-18T23:29:56.373908Z","iopub.status.idle":"2025-11-18T23:31:09.991127Z","shell.execute_reply.started":"2025-11-18T23:29:56.373875Z","shell.execute_reply":"2025-11-18T23:31:09.990326Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.10.5)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nUsing cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\nUsing cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\nUsing cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\nUsing cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\nDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":3},{"id":"44b4809d-805d-45ae-b5f6-d484d1ff4e8d","cell_type":"code","source":"import os\nimport json\nimport textwrap\nimport bitsandbytes \nfrom typing import Tuple, Optional\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# with open(\"secrets.json\", \"r\", encoding=\"utf-8\") as f:\n#     secrets = json.load(f)\n#HF_TOKEN = secrets[\"huggingface_token\"]\n\n#HF_TOKEN =  en entorno kagle escribir aquí \n# Prompt builder\ndef random_applicant() -> dict:\n    \"\"\"Genera un perfil de solicitante aleatorio (en inglés, con más variedad y contexto).\"\"\"\n    import random\n\n    # --- Datos básicos ---\n    first_names = [\n        \"Alex\", \"Jamie\", \"Taylor\", \"Jordan\", \"Morgan\", \"Riley\", \"Casey\", \"Chris\", \"Avery\", \"Drew\",\n        \"Samantha\", \"Daniel\", \"Sophia\", \"Michael\", \"Olivia\", \"Liam\", \"Isabella\", \"Noah\", \"Ethan\", \"Emma\"\n    ]\n    last_names = [\n        \"Johnson\", \"Smith\", \"Lee\", \"Patel\", \"Williams\", \"Garcia\", \"Brown\", \"Davis\", \"Miller\", \"Wilson\",\n        \"Martinez\", \"Anderson\", \"Clark\", \"Lopez\", \"Lewis\", \"Walker\", \"Young\", \"Allen\", \"King\"\n    ]\n\n    # --- Roles profesionales ---\n    roles = [\n        \"Data Scientist\", \"Software Engineer\", \"Frontend Developer\", \"UX Designer\",\n        \"Marketing Manager\", \"Business Analyst\", \"Product Manager\", \"Research Scientist\",\n        \"Sales Executive\", \"Customer Success Manager\", \"Financial Analyst\", \"HR Coordinator\",\n        \"Operations Lead\", \"IT Support Specialist\", \"DevOps Engineer\", \"Content Strategist\",\n        \"Mechanical Engineer\", \"Project Manager\", \"Copywriter\", \"Legal Assistant\"\n    ]\n\n    # --- Empresas ficticias ---\n    companies = [\n        \"Acme Corp\", \"TechNova\", \"BlueSky Labs\", \"FutureWorks\", \"Quantum Analytics\", \"NextGen Solutions\",\n        \"EverBright\", \"Pioneer Systems\", \"OrbitSoft\", \"NeuralEdge\", \"BrightLeaf Consulting\", \"SkyBridge AI\",\n        \"Helix Dynamics\", \"CleverPath\", \"OptiData\", \"Sunrise Media\", \"GreenFlow Technologies\"\n    ]\n\n    # --- Sectores o industrias ---\n    industries = [\n        \"technology\", \"finance\", \"education\", \"healthcare\", \"energy\", \"marketing\", \"consulting\",\n        \"manufacturing\", \"media\", \"logistics\", \"environmental science\", \"AI research\", \"non-profit\"\n    ]\n\n    # --- Nivel educativo / académico ---\n    education_levels = [\n        \"Bachelor's in Computer Science\", \"Master's in Data Analytics\", \"MBA\",\n        \"Bachelor's in Marketing\", \"PhD in Artificial Intelligence\", \"Bachelor's in Business Administration\",\n        \"Master's in Mechanical Engineering\", \"Bachelor's in Graphic Design\", \"BSc in Economics\",\n        \"Bachelor's in Psychology\", \"BA in Communications\"\n    ]\n\n    # --- Tonos posibles ---\n    tones = [\n        \"professional\", \"enthusiastic\", \"confident\", \"humble but ambitious\", \"creative\", \"friendly\", \"formal\",\n        \"inspirational\", \"passionate about innovation\", \"results-driven\", \"empathetic\", \"analytical\"\n    ]\n\n    # --- Habilidades ---\n    skills_pool = [\n        \"Python\", \"R\", \"Java\", \"C++\", \"SQL\", \"Machine Learning\", \"Deep Learning\", \"Project Management\",\n        \"Marketing Strategy\", \"Data Visualization\", \"Cloud Computing\", \"Communication\", \"Leadership\",\n        \"Team Collaboration\", \"Research\", \"Excel\", \"Power BI\", \"TensorFlow\", \"React\", \"Docker\", \"Kubernetes\",\n        \"SEO Optimization\", \"Public Speaking\", \"Customer Relations\", \"Financial Modeling\"\n    ]\n\n    # --- Extras que afectan el prompt ---\n    extras = [\n        \"Emphasize measurable impact and leadership.\",\n        \"Highlight adaptability and eagerness to learn new tools.\",\n        \"Show strong cross-functional collaboration and communication.\",\n        \"Focus on creative problem-solving and curiosity for innovation.\",\n        \"Mention the applicant’s commitment to diversity and inclusion.\",\n        \"Emphasize attention to detail and efficiency.\",\n        \"Showcase ability to work under pressure and meet deadlines.\",\n        \"Highlight mentorship and team motivation skills.\",\n        \"Include a brief anecdote that reflects passion for the field.\",\n        \"Keep the tone optimistic and forward-thinking.\"\n    ]\n\n    # --- Generación aleatoria ---\n    name = f\"{random.choice(first_names)} {random.choice(last_names)}\"\n    role = random.choice(roles)\n    company = random.choice(companies)\n    tone = random.choice(tones)\n    industry = random.choice(industries)\n    education_level = random.choice(education_levels)\n    extra = random.choice(extras)\n\n    n_skills = random.randint(4, 7)\n    skills_list = random.sample(skills_pool, n_skills)\n\n    exp_years = random.randint(1, 15)\n    exp_focus = random.choice([\n        f\"developing {role.lower()} solutions for the {industry} sector\",\n        f\"driving innovation and data-driven decisions\",\n        f\"leading multidisciplinary teams in fast-paced environments\",\n        f\"managing end-to-end projects and achieving KPIs\",\n        f\"improving processes and optimizing efficiency\"\n    ])\n\n    experience_summary = f\"{exp_years} years of experience {exp_focus}.\"\n\n    return {\n        \"name\": name,\n        \"role\": role,\n        \"company\": company,\n        \"industry\": industry,\n        \"education_level\": education_level,\n        \"experience_summary\": experience_summary,\n        \"skills_list\": skills_list,\n        \"tone\": tone,\n        \"extra\": extra\n    }\n\n\ndef build_cover_letter_prompt(applicant: dict) -> str:\n    \"\"\"Construye prompt consistente para generar la cover letter (en inglés).\"\"\"\n    skills = \", \".join(applicant.get(\"skills_list\", [])) or \"N/A\"\n    extra = applicant.get(\"extra\", \"\")\n    prompt = f\"\"\"\nYou are an expert career advisor and professional cover letter writer.\n\nWrite a concise, persuasive English cover letter (job application / cover letter) using the information below.\nReturn only the letter body (no metadata or commentary).\n\nApplicant:\n- Name: {applicant.get('name', 'Applicant')}\n- Target role: {applicant.get('role', '')}\n- Target company: {applicant.get('company', '')}\n- Short experience summary: {applicant.get('experience_summary', '')}\n- Key skills: {skills}\n- Tone: {applicant.get('tone', 'professional')}\n- Extra: {extra}\n\nRequirements:\n- Length: 150-300 words.\n- Include a tailored opening line mentioning the company and role.\n- Include 2-3 sentences highlighting accomplishments/results (use approximate numbers if necessary).\n- Use a clear closing paragraph inviting next steps and a polite sign-off.\n- Do not include real contact details; use placeholders if needed.\n\nBegin the letter with an appropriate greeting (e.g., \"Dear Hiring Manager,\" or \"Dear <Company> Recruiting Team,\").\n\"\"\"\n    return textwrap.dedent(prompt).strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T23:31:09.992623Z","iopub.execute_input":"2025-11-18T23:31:09.993267Z","iopub.status.idle":"2025-11-18T23:31:23.472130Z","shell.execute_reply.started":"2025-11-18T23:31:09.993237Z","shell.execute_reply":"2025-11-18T23:31:23.471462Z"}},"outputs":[{"name":"stderr","text":"bitsandbytes library load error: Configured CUDA binary not found at /kaggle/working/bitsandbytes/bitsandbytes/libbitsandbytes_cuda124.so\nTraceback (most recent call last):\n  File \"/kaggle/working/bitsandbytes/bitsandbytes/cextension.py\", line 320, in <module>\n    lib = get_native_library()\n          ^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/bitsandbytes/bitsandbytes/cextension.py\", line 288, in get_native_library\n    raise RuntimeError(f\"Configured {BNB_BACKEND} binary not found at {cuda_binary_path}\")\nRuntimeError: Configured CUDA binary not found at /kaggle/working/bitsandbytes/bitsandbytes/libbitsandbytes_cuda124.so\n","output_type":"stream"}],"execution_count":4},{"id":"92a3ad20-69bc-47e8-8b6b-12f1d7fb345f","cell_type":"code","source":"# Ajustes: cambia el nombre del modelo según lo que tengas disponible localmente\n# Para Llama2 en HF: \"meta-llama/Llama-2-7b-chat-hf\"\nMODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n\nload_in_4bit = 0\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\"\n)\n\n# Opción A: intentar cargar en 4-bit (recomendado si bitsandbytes soporta tu entorno)\nif load_in_4bit:\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        device_map=\"auto\",\n        quantization_config=bnb_config,\n        torch_dtype=\"auto\",\n        use_auth_token=HF_TOKEN\n    )\n    print(\"cargado 4bits\")\nelse:\n    # Opción B: cargar en 8-bit\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        device_map=\"auto\",\n        load_in_8bit=True,\n        torch_dtype=\"auto\",\n        token=HF_TOKEN\n    )\n    print(\"cargado 8bits\")\n    \ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,use_auth_token=HF_TOKEN)\n\n# asegúrate de que tokenizer.eos_token_id existe\nif tokenizer.eos_token_id is None:\n    tokenizer.eos_token_id = tokenizer.pad_token_id or 0\n\n#base\ngen_kwargs = {\n        \"max_new_tokens\": 300,\n        \"do_sample\": True,\n        \"temperature\": 0.7,\n        \"top_p\": 0.95,\n        \"repetition_penalty\": 1.05,\n        \"pad_token_id\": tokenizer.eos_token_id}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T23:31:23.472773Z","iopub.execute_input":"2025-11-18T23:31:23.473100Z","iopub.status.idle":"2025-11-18T23:31:23.577213Z","shell.execute_reply.started":"2025-11-18T23:31:23.473083Z","shell.execute_reply":"2025-11-18T23:31:23.576308Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mfrom_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mStopIteration\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/1839830199.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mload_in_4bit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m bnb_config = BitsAndBytesConfig(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbnb_4bit_use_double_quant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/quantization_config.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load_in_8bit, load_in_4bit, llm_int8_threshold, llm_int8_skip_modules, llm_int8_enable_fp32_cpu_offload, llm_int8_has_fp16_weight, bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant, bnb_4bit_quant_storage, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unused kwargs: {list(kwargs.keys())}. These kwargs are not used in {self.__class__}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/quantization_config.py\u001b[0m in \u001b[0;36mpost_init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bnb_4bit_use_double_quant must be a boolean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         if self.load_in_4bit and not version.parse(importlib.metadata.version(\"bitsandbytes\")) >= version.parse(\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0;34m\"0.39.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         ):\n","\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0;34m\"Version\"\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \"\"\"\n\u001b[0;32m-> 1009\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistribution_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDistribution\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mor\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mthereof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m     \"\"\"\n\u001b[0;32m--> 982\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistribution_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mfrom_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPackageNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPackageNotFoundError\u001b[0m: No package metadata was found for bitsandbytes"],"ename":"PackageNotFoundError","evalue":"No package metadata was found for bitsandbytes","output_type":"error"}],"execution_count":5},{"id":"ac7f8f84-136a-4c54-aa5d-aa55589e922a","cell_type":"code","source":"def generate(prompt, model=model, tokenizer=tokenizer, gen_kwargs=gen_kwargs):\n    \"\"\"\n    model: modelo cargado\n    tokenizer: tokenizer\n    prompt: string\n    gen_kwargs: dict con argumentos para model.generate()\n    \"\"\"\n\n    # tokenizar prompt\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    # generar\n    out = model.generate(**inputs, **gen_kwargs)\n\n    # extraer solo lo generado\n    start_len = inputs[\"input_ids\"].shape[1]\n    generated_ids = out[0][start_len:]\n    text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n\n    return text\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a8579798-9fd7-4a9b-9fa3-c3bd59d9494e","cell_type":"code","source":"def generate_n_cover_letters(n: int, model, tokenizer, output_file=\"cover_letters.jsonl\"):\n\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        for i in range(n):\n            applicant = random_applicant()\n            prompt = build_cover_letter_prompt(applicant)\n\n            letter = generate(\n                model=model,\n                tokenizer=tokenizer,\n                prompt=prompt,\n                gen_kwargs=gen_kwargs\n            )\n\n            # guardar en JSONL\n            record = {\n                \"applicant\": applicant,\n                \"cover_letter\": letter\n            }\n\n            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n            print(f\"[+] Generada carta {i+1}/{n}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"fef0ba98-6404-464e-ac53-52ae18d381c5","cell_type":"markdown","source":"### abajo basura","metadata":{}},{"id":"76597482","cell_type":"code","source":"#  Ejemplo de uso \n\nsample_applicant = {\n    \"name\": \"Alex Johnson\",\n    \"role\": \"Data Scientist\",\n    \"company\": \"Acme Tech\",\n    \"experience_summary\": \"3 years building production ML pipelines and improving prediction accuracy for customer churn.\",\n    \"skills_list\": [\"Python\", \"pandas\", \"scikit-learn\", \"SQL\", \"model deployment\"],\n    \"tone\": \"professional and confident\",\n    \"extra\": \"Emphasize cross-team collaboration and measurable impact.\"\n}\n\n# Genera usando LLaMA local (intenta 8-bit)\ntry:\n    llm_cover = generate_cover_llama_local(sample_applicant, use_8bit=True, temperature=0.7)\n    print(\"\\n--- LLaMA COVER ---\\n\", llm_cover[:2000])\nexcept Exception as e:\n    LOG.error(\"LLaMA generation failed: %s\", e)\n\n# Genera usando HF model local (p. ej. Falcon)\ntry:\n    hf_cover = generate_cover_hf_local(sample_applicant, use_8bit=True, temperature=0.9)\n    print(\"\\n--- HF COVER ---\\n\", hf_cover[:2000])\nexcept Exception as e:\n    LOG.error(\"HF generation failed: %s\", e)\n","metadata":{},"outputs":[],"execution_count":null},{"id":"5e2f8c9e","cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\n\ndef generate_cover_dataset(\n    n: int = 10,\n    model_type: str = \"llama\",  # o \"hf\"\n    temperature: float = 0.7,\n    use_8bit: bool = True,\n    use_4bit: bool = False,\n    secrets_path: str = \"secrets.json\",\n    save_path: Optional[str] = None,\n) -> pd.DataFrame:\n    \"\"\"\n    Genera un dataset de cover letters con prompts y resultados.\n    model_type: \"llama\" o \"hf\"\n    \"\"\"\n    data = []\n\n    LOG.info(f\"Generating {n} cover letters using model_type={model_type}...\")\n\n    for _ in tqdm(range(n), desc=\"Generating covers\"):\n        applicant = random_applicant()\n        prompt = build_cover_letter_prompt(applicant)\n\n        try:\n            if model_type.lower() == \"llama\":\n                letter = generate_cover_llama_local(\n                    applicant,\n                    secrets_path=secrets_path,\n                    use_8bit=use_8bit,\n                    use_4bit=use_4bit,\n                    temperature=temperature,\n                )\n            else:\n                letter = generate_cover_hf_local(\n                    applicant,\n                    secrets_path=secrets_path,\n                    use_8bit=use_8bit,\n                    use_4bit=use_4bit,\n                    temperature=temperature,\n                )\n        except Exception as e:\n            LOG.error(\"Error generating letter: %s\", e)\n            letter = f\"[ERROR] {e}\"\n\n        row = {\n            **applicant,\n            \"prompt\": prompt,\n            \"cover_letter\": letter,\n        }\n        data.append(row)\n\n    df = pd.DataFrame(data)\n    LOG.info(\" Dataset generated with %d samples\", len(df))\n\n    if save_path:\n        df.to_csv(save_path, index=False)\n        LOG.info(\"Dataset saved to %s\", save_path)\n\n    return df\n","metadata":{},"outputs":[],"execution_count":null}]}