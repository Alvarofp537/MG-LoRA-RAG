{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e6f67e",
   "metadata": {},
   "source": [
    "Implementamos un generador de datos sinteticos basado en un promt generado también de forma sintética para conseguir más diversidad de los textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf945111",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate bitsandbytes sentencepiece huggingface-hub torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2fd5ff",
   "metadata": {},
   "source": [
    "\n",
    "Generación local de cover letters usando modelos descargados desde Hugging Face<br>\n",
    "(soporta quantización 8-bit via bitsandbytes).\n",
    "\n",
    "Uso:<br>\n",
    "  - Crea un secrets.json con tu token HF si el modelo requiere autenticación:<br>\n",
    "    {\n",
    "      \"HUGGINGFACE_TOKEN\": \"hf_xxx\",\n",
    "\n",
    "      \"LLAMA_MODEL\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "      \n",
    "      \"HF_MODEL\": \"tiiuae/falcon-7b-instruct\"\n",
    "    }\n",
    "  - Añade secrets.json a .gitignore antes de subir a GitHub.\n",
    "\n",
    "Notas de hardware:\n",
    "  - En GPU tipo P100 (12/16GB VRAM) modelos de ~7B con 8-bit suelen ser factibles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b1861",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "import logging\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "LOG = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "#Utils \n",
    "def load_secrets(secrets_path: str = \"secrets.json\") -> dict:\n",
    "    \"\"\"Carga secrets.json si existe\"\"\"\n",
    "    if os.path.exists(secrets_path):\n",
    "        with open(secrets_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "\n",
    "# Prompt builder\n",
    "def random_applicant() -> dict:\n",
    "    \"\"\"Genera un perfil de solicitante aleatorio (en inglés, con más variedad y contexto).\"\"\"\n",
    "    import random\n",
    "\n",
    "    # --- Datos básicos ---\n",
    "    first_names = [\n",
    "        \"Alex\", \"Jamie\", \"Taylor\", \"Jordan\", \"Morgan\", \"Riley\", \"Casey\", \"Chris\", \"Avery\", \"Drew\",\n",
    "        \"Samantha\", \"Daniel\", \"Sophia\", \"Michael\", \"Olivia\", \"Liam\", \"Isabella\", \"Noah\", \"Ethan\", \"Emma\"\n",
    "    ]\n",
    "    last_names = [\n",
    "        \"Johnson\", \"Smith\", \"Lee\", \"Patel\", \"Williams\", \"Garcia\", \"Brown\", \"Davis\", \"Miller\", \"Wilson\",\n",
    "        \"Martinez\", \"Anderson\", \"Clark\", \"Lopez\", \"Lewis\", \"Walker\", \"Young\", \"Allen\", \"King\"\n",
    "    ]\n",
    "\n",
    "    # --- Roles profesionales ---\n",
    "    roles = [\n",
    "        \"Data Scientist\", \"Software Engineer\", \"Frontend Developer\", \"UX Designer\",\n",
    "        \"Marketing Manager\", \"Business Analyst\", \"Product Manager\", \"Research Scientist\",\n",
    "        \"Sales Executive\", \"Customer Success Manager\", \"Financial Analyst\", \"HR Coordinator\",\n",
    "        \"Operations Lead\", \"IT Support Specialist\", \"DevOps Engineer\", \"Content Strategist\",\n",
    "        \"Mechanical Engineer\", \"Project Manager\", \"Copywriter\", \"Legal Assistant\"\n",
    "    ]\n",
    "\n",
    "    # --- Empresas ficticias ---\n",
    "    companies = [\n",
    "        \"Acme Corp\", \"TechNova\", \"BlueSky Labs\", \"FutureWorks\", \"Quantum Analytics\", \"NextGen Solutions\",\n",
    "        \"EverBright\", \"Pioneer Systems\", \"OrbitSoft\", \"NeuralEdge\", \"BrightLeaf Consulting\", \"SkyBridge AI\",\n",
    "        \"Helix Dynamics\", \"CleverPath\", \"OptiData\", \"Sunrise Media\", \"GreenFlow Technologies\"\n",
    "    ]\n",
    "\n",
    "    # --- Sectores o industrias ---\n",
    "    industries = [\n",
    "        \"technology\", \"finance\", \"education\", \"healthcare\", \"energy\", \"marketing\", \"consulting\",\n",
    "        \"manufacturing\", \"media\", \"logistics\", \"environmental science\", \"AI research\", \"non-profit\"\n",
    "    ]\n",
    "\n",
    "    # --- Nivel educativo / académico ---\n",
    "    education_levels = [\n",
    "        \"Bachelor's in Computer Science\", \"Master's in Data Analytics\", \"MBA\",\n",
    "        \"Bachelor's in Marketing\", \"PhD in Artificial Intelligence\", \"Bachelor's in Business Administration\",\n",
    "        \"Master's in Mechanical Engineering\", \"Bachelor's in Graphic Design\", \"BSc in Economics\",\n",
    "        \"Bachelor's in Psychology\", \"BA in Communications\"\n",
    "    ]\n",
    "\n",
    "    # --- Tonos posibles ---\n",
    "    tones = [\n",
    "        \"professional\", \"enthusiastic\", \"confident\", \"humble but ambitious\", \"creative\", \"friendly\", \"formal\",\n",
    "        \"inspirational\", \"passionate about innovation\", \"results-driven\", \"empathetic\", \"analytical\"\n",
    "    ]\n",
    "\n",
    "    # --- Habilidades ---\n",
    "    skills_pool = [\n",
    "        \"Python\", \"R\", \"Java\", \"C++\", \"SQL\", \"Machine Learning\", \"Deep Learning\", \"Project Management\",\n",
    "        \"Marketing Strategy\", \"Data Visualization\", \"Cloud Computing\", \"Communication\", \"Leadership\",\n",
    "        \"Team Collaboration\", \"Research\", \"Excel\", \"Power BI\", \"TensorFlow\", \"React\", \"Docker\", \"Kubernetes\",\n",
    "        \"SEO Optimization\", \"Public Speaking\", \"Customer Relations\", \"Financial Modeling\"\n",
    "    ]\n",
    "\n",
    "    # --- Extras que afectan el prompt ---\n",
    "    extras = [\n",
    "        \"Emphasize measurable impact and leadership.\",\n",
    "        \"Highlight adaptability and eagerness to learn new tools.\",\n",
    "        \"Show strong cross-functional collaboration and communication.\",\n",
    "        \"Focus on creative problem-solving and curiosity for innovation.\",\n",
    "        \"Mention the applicant’s commitment to diversity and inclusion.\",\n",
    "        \"Emphasize attention to detail and efficiency.\",\n",
    "        \"Showcase ability to work under pressure and meet deadlines.\",\n",
    "        \"Highlight mentorship and team motivation skills.\",\n",
    "        \"Include a brief anecdote that reflects passion for the field.\",\n",
    "        \"Keep the tone optimistic and forward-thinking.\"\n",
    "    ]\n",
    "\n",
    "    # --- Generación aleatoria ---\n",
    "    name = f\"{random.choice(first_names)} {random.choice(last_names)}\"\n",
    "    role = random.choice(roles)\n",
    "    company = random.choice(companies)\n",
    "    tone = random.choice(tones)\n",
    "    industry = random.choice(industries)\n",
    "    education_level = random.choice(education_levels)\n",
    "    extra = random.choice(extras)\n",
    "\n",
    "    n_skills = random.randint(4, 7)\n",
    "    skills_list = random.sample(skills_pool, n_skills)\n",
    "\n",
    "    exp_years = random.randint(1, 15)\n",
    "    exp_focus = random.choice([\n",
    "        f\"developing {role.lower()} solutions for the {industry} sector\",\n",
    "        f\"driving innovation and data-driven decisions\",\n",
    "        f\"leading multidisciplinary teams in fast-paced environments\",\n",
    "        f\"managing end-to-end projects and achieving KPIs\",\n",
    "        f\"improving processes and optimizing efficiency\"\n",
    "    ])\n",
    "\n",
    "    experience_summary = f\"{exp_years} years of experience {exp_focus}.\"\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"role\": role,\n",
    "        \"company\": company,\n",
    "        \"industry\": industry,\n",
    "        \"education_level\": education_level,\n",
    "        \"experience_summary\": experience_summary,\n",
    "        \"skills_list\": skills_list,\n",
    "        \"tone\": tone,\n",
    "        \"extra\": extra\n",
    "    }\n",
    "\n",
    "\n",
    "def build_cover_letter_prompt(applicant: dict) -> str:\n",
    "    \"\"\"Construye prompt consistente para generar la cover letter (en inglés).\"\"\"\n",
    "    skills = \", \".join(applicant.get(\"skills_list\", [])) or \"N/A\"\n",
    "    extra = applicant.get(\"extra\", \"\")\n",
    "    prompt = f\"\"\"\n",
    "You are an expert career advisor and professional cover letter writer.\n",
    "\n",
    "Write a concise, persuasive English cover letter (job application / cover letter) using the information below.\n",
    "Return only the letter body (no metadata or commentary).\n",
    "\n",
    "Applicant:\n",
    "- Name: {applicant.get('name', 'Applicant')}\n",
    "- Target role: {applicant.get('role', '')}\n",
    "- Target company: {applicant.get('company', '')}\n",
    "- Short experience summary: {applicant.get('experience_summary', '')}\n",
    "- Key skills: {skills}\n",
    "- Tone: {applicant.get('tone', 'professional')}\n",
    "- Extra: {extra}\n",
    "\n",
    "Requirements:\n",
    "- Length: 150-300 words.\n",
    "- Include a tailored opening line mentioning the company and role.\n",
    "- Include 2-3 sentences highlighting accomplishments/results (use approximate numbers if necessary).\n",
    "- Use a clear closing paragraph inviting next steps and a polite sign-off.\n",
    "- Do not include real contact details; use placeholders if needed.\n",
    "\n",
    "Begin the letter with an appropriate greeting (e.g., \"Dear Hiring Manager,\" or \"Dear <Company> Recruiting Team,\").\n",
    "\"\"\"\n",
    "    return textwrap.dedent(prompt).strip()\n",
    "\n",
    "\n",
    "# Model loading (local) \n",
    "def load_model_local(\n",
    "    model_name: str,\n",
    "    hf_token: Optional[str] = None,\n",
    "    use_8bit: bool = True,\n",
    "    use_4bit: bool = False,\n",
    ") -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"\n",
    "    Intenta cargar un modelo localmente.\n",
    "    - model_name: repo HF (ej: \"meta-llama/Llama-2-7b-chat-hf\" o \"tiiuae/falcon-7b-instruct\")\n",
    "    - hf_token: token de Hugging Face if needed (o configura HF_TOKEN env)\n",
    "    - use_8bit: intenta load_in_8bit (bitsandbytes)\n",
    "    - use_4bit: alternativa, usa load_in_4bit (bitsandbytes + soporte)\n",
    "    Retorna (model, tokenizer).\n",
    "    \"\"\"\n",
    "    # token fallback\n",
    "    hf_token = hf_token or os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "    LOG.info(\"Cargando tokenizer para %s ...\", model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        use_fast=False,\n",
    "        trust_remote_code=True,\n",
    "        use_auth_token=hf_token,\n",
    "    )\n",
    "\n",
    "    # Intentar carga con quantización (8-bit)\n",
    "    load_kwargs = {\"trust_remote_code\": True, \"use_auth_token\": hf_token}\n",
    "    try:\n",
    "        if use_4bit:\n",
    "            LOG.info(\"Intentando cargar en 4-bit (bitsandbytes)...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                load_in_4bit=True,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16,\n",
    "                **load_kwargs,\n",
    "            )\n",
    "        elif use_8bit:\n",
    "            LOG.info(\"Intentando cargar en 8-bit (bitsandbytes)...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                load_in_8bit=True,\n",
    "                device_map=\"auto\",\n",
    "                **load_kwargs,\n",
    "            )\n",
    "        else:\n",
    "            LOG.info(\"Cargando en precisión normal (puede OOM)...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else None,\n",
    "                **load_kwargs,\n",
    "            )\n",
    "        LOG.info(\"Modelo cargado correctamente: %s\", model_name)\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        LOG.warning(\"Carga con quantización falló: %s\", e)\n",
    "        LOG.info(\"Intentando carga sin quantización (fallback)...\")\n",
    "        # fallback sin quantización (unfeasible because of size)\n",
    "        try:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else None,\n",
    "                trust_remote_code=True,\n",
    "                use_auth_token=hf_token,\n",
    "            )\n",
    "            LOG.info(\"Modelo cargado en fallback (sin quantización).\")\n",
    "            return model, tokenizer\n",
    "        except Exception as e2:\n",
    "            LOG.error(\"Carga fallback falló: %s\", e2)\n",
    "            raise RuntimeError(f\"Fallo al cargar modelo local `{model_name}`: {e2}\")\n",
    "\n",
    "\n",
    "#  Generation helper \n",
    "def generate_with_model(\n",
    "    applicant: dict,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    temperature: float = 0.7,\n",
    "    max_new_tokens: int = 400,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Genera cover letter usando model/tokenizer ya cargados.\n",
    "    - applicant: dict con keys (name, role, company, experience_summary, skills_list, tone, extra)\n",
    "    - model: AutoModelForCausalLM\n",
    "    - tokenizer: AutoTokenizer\n",
    "    \"\"\"\n",
    "    prompt = build_cover_letter_prompt(applicant)\n",
    "    # tokenizar\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    # mover a device del modelo si existe mapping (model.device_map) o a cuda/CPU\n",
    "    try:\n",
    "        # Si model tiene atributo device_map (hf accelerate), buscar device del primer parámetro\n",
    "        if hasattr(model, \"device\"):\n",
    "            device = model.device\n",
    "        else:\n",
    "            # usa cuda si disponible\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    except Exception:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Config de generación (usamos GenerationConfig si el modelo lo soporta)\n",
    "    gen_kwargs = dict(\n",
    "        temperature=float(temperature),\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=int(max_new_tokens),\n",
    "    )\n",
    "    try:\n",
    "        # Si el modelo soporta GenerationConfig (nuevo API)\n",
    "        if \"GenerationConfig\" in globals():\n",
    "            generation_config = GenerationConfig(**gen_kwargs)\n",
    "            outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "        else:\n",
    "            outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    except TypeError:\n",
    "        # fallback a call directo\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Si el modelo repite el prompt, intentamos quitar el prompt inicial\n",
    "    if text.startswith(prompt):\n",
    "        text = text[len(prompt):].strip()\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "#  Wrappers solicitados \n",
    "# Mantener cache para no recargar el modelo cada llamada\n",
    "_MODEL_CACHE = {}\n",
    "\n",
    "\n",
    "def generate_cover_llama_local(\n",
    "    applicant: dict,\n",
    "    secrets_path: str = \"secrets.json\",\n",
    "    model_name: Optional[str] = None,\n",
    "    use_8bit: bool = True,\n",
    "    use_4bit: bool = False,\n",
    "    temperature: float = 0.7,\n",
    "    max_new_tokens: int = 400,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Genera cover letter con un modelo LLaMA local (intenta quantización).\n",
    "    - Lee secrets.json para token HF si es necesario.\n",
    "    - model_name por defecto se toma de secrets o se fija a Llama-2-7b-chat-hf.\n",
    "    \"\"\"\n",
    "    secrets = load_secrets(secrets_path)\n",
    "    hf_token = secrets.get(\"HUGGINGFACE_TOKEN\") or os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "    model_name = model_name or secrets.get(\"LLAMA_MODEL\") or \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "    cache_key = f\"llama::{model_name}::8b{use_8bit}::4b{use_4bit}\"\n",
    "    if cache_key not in _MODEL_CACHE:\n",
    "        model, tokenizer = load_model_local(model_name, hf_token=hf_token, use_8bit=use_8bit, use_4bit=use_4bit)\n",
    "        _MODEL_CACHE[cache_key] = (model, tokenizer)\n",
    "    else:\n",
    "        model, tokenizer = _MODEL_CACHE[cache_key]\n",
    "\n",
    "    return generate_with_model(applicant, model, tokenizer, temperature=temperature, max_new_tokens=max_new_tokens)\n",
    "\n",
    "\n",
    "def generate_cover_hf_local(\n",
    "    applicant: dict,\n",
    "    secrets_path: str = \"secrets.json\",\n",
    "    model_name: Optional[str] = None,\n",
    "    use_8bit: bool = True,\n",
    "    use_4bit: bool = False,\n",
    "    temperature: float = 0.7,\n",
    "    max_new_tokens: int = 400,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Genera cover letter con un modelo de Hugging Face local (ej: Falcon).\n",
    "    Funciona igual que generate_cover_llama_local, distinto default model.\n",
    "    \"\"\"\n",
    "    secrets = load_secrets(secrets_path)\n",
    "    hf_token = secrets.get(\"HUGGINGFACE_TOKEN\") or os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "    model_name = model_name or secrets.get(\"HF_MODEL\") or \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "    cache_key = f\"hf::{model_name}::8b{use_8bit}::4b{use_4bit}\"\n",
    "    if cache_key not in _MODEL_CACHE:\n",
    "        model, tokenizer = load_model_local(model_name, hf_token=hf_token, use_8bit=use_8bit, use_4bit=use_4bit)\n",
    "        _MODEL_CACHE[cache_key] = (model, tokenizer)\n",
    "    else:\n",
    "        model, tokenizer = _MODEL_CACHE[cache_key]\n",
    "\n",
    "    return generate_with_model(applicant, model, tokenizer, temperature=temperature, max_new_tokens=max_new_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76597482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Ejemplo de uso \n",
    "\n",
    "sample_applicant = {\n",
    "    \"name\": \"Alex Johnson\",\n",
    "    \"role\": \"Data Scientist\",\n",
    "    \"company\": \"Acme Tech\",\n",
    "    \"experience_summary\": \"3 years building production ML pipelines and improving prediction accuracy for customer churn.\",\n",
    "    \"skills_list\": [\"Python\", \"pandas\", \"scikit-learn\", \"SQL\", \"model deployment\"],\n",
    "    \"tone\": \"professional and confident\",\n",
    "    \"extra\": \"Emphasize cross-team collaboration and measurable impact.\"\n",
    "}\n",
    "\n",
    "# Genera usando LLaMA local (intenta 8-bit)\n",
    "try:\n",
    "    llm_cover = generate_cover_llama_local(sample_applicant, use_8bit=True, temperature=0.7)\n",
    "    print(\"\\n--- LLaMA COVER ---\\n\", llm_cover[:2000])\n",
    "except Exception as e:\n",
    "    LOG.error(\"LLaMA generation failed: %s\", e)\n",
    "\n",
    "# Genera usando HF model local (p. ej. Falcon)\n",
    "try:\n",
    "    hf_cover = generate_cover_hf_local(sample_applicant, use_8bit=True, temperature=0.9)\n",
    "    print(\"\\n--- HF COVER ---\\n\", hf_cover[:2000])\n",
    "except Exception as e:\n",
    "    LOG.error(\"HF generation failed: %s\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f8c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_cover_dataset(\n",
    "    n: int = 10,\n",
    "    model_type: str = \"llama\",  # o \"hf\"\n",
    "    temperature: float = 0.7,\n",
    "    use_8bit: bool = True,\n",
    "    use_4bit: bool = False,\n",
    "    secrets_path: str = \"secrets.json\",\n",
    "    save_path: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Genera un dataset de cover letters con prompts y resultados.\n",
    "    model_type: \"llama\" o \"hf\"\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    LOG.info(f\"Generating {n} cover letters using model_type={model_type}...\")\n",
    "\n",
    "    for _ in tqdm(range(n), desc=\"Generating covers\"):\n",
    "        applicant = random_applicant()\n",
    "        prompt = build_cover_letter_prompt(applicant)\n",
    "\n",
    "        try:\n",
    "            if model_type.lower() == \"llama\":\n",
    "                letter = generate_cover_llama_local(\n",
    "                    applicant,\n",
    "                    secrets_path=secrets_path,\n",
    "                    use_8bit=use_8bit,\n",
    "                    use_4bit=use_4bit,\n",
    "                    temperature=temperature,\n",
    "                )\n",
    "            else:\n",
    "                letter = generate_cover_hf_local(\n",
    "                    applicant,\n",
    "                    secrets_path=secrets_path,\n",
    "                    use_8bit=use_8bit,\n",
    "                    use_4bit=use_4bit,\n",
    "                    temperature=temperature,\n",
    "                )\n",
    "        except Exception as e:\n",
    "            LOG.error(\"Error generating letter: %s\", e)\n",
    "            letter = f\"[ERROR] {e}\"\n",
    "\n",
    "        row = {\n",
    "            **applicant,\n",
    "            \"prompt\": prompt,\n",
    "            \"cover_letter\": letter,\n",
    "        }\n",
    "        data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    LOG.info(\" Dataset generated with %d samples\", len(df))\n",
    "\n",
    "    if save_path:\n",
    "        df.to_csv(save_path, index=False)\n",
    "        LOG.info(\"Dataset saved to %s\", save_path)\n",
    "\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
